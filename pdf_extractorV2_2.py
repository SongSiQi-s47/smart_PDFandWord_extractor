# -*- coding: utf-8 -*-
"""
PDFå’ŒWordè¡¨æ ¼æå–å·¥å…· - Webç‰ˆæœ¬
ä¿æŒæœ¬åœ°ç‰ˆçš„æ‰€æœ‰åŠŸèƒ½ï¼Œåªå»æ‰input()å‡½æ•°
"""

import os
import re
import logging
import pdfplumber
import pandas as pd
from docx import Document
from typing import List, Dict, Optional, Tuple
from io import BytesIO
import openpyxl
from openpyxl.styles import Font, Alignment, Border, Side

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_sample_to_template(sample):
    """å°†æ ·ä¾‹è½¬æ¢ä¸ºæ¨¡æ¿"""
    if not sample:
        return ""
    
    # æ›¿æ¢æ•°å­—ä¸ºå ä½ç¬¦
    template = re.sub(r'\d+', 'N', sample)
    # æ›¿æ¢å­—æ¯ä¸ºå ä½ç¬¦
    template = re.sub(r'[a-zA-Z]+', 'L', template)
    return template

def template_to_regex(template):
    """å°†æ¨¡æ¿è½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼"""
    if not template:
        return ""
    
    # æ›¿æ¢å ä½ç¬¦ä¸ºå¯¹åº”çš„æ­£åˆ™è¡¨è¾¾å¼
    regex = template.replace('N', r'\d+')
    regex = regex.replace('L', r'[a-zA-Z]+')
    # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
    regex = re.escape(regex)
    # æ¢å¤å ä½ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
    regex = regex.replace(r'\\N', r'\d+')
    regex = regex.replace(r'\\L', r'[a-zA-Z]+')
    
    return f"^{regex}$"

def get_regex_from_sample(sample):
    """ä»æ ·ä¾‹ç›´æ¥è·å–æ­£åˆ™è¡¨è¾¾å¼"""
    template = parse_sample_to_template(sample)
    return template_to_regex(template)

def get_fuzzy_regex_from_sample(sample):
    """è·å–æ¨¡ç³ŠåŒ¹é…çš„æ­£åˆ™è¡¨è¾¾å¼"""
    if not sample:
        return ""
    
    # åˆ›å»ºæ›´å®½æ¾çš„åŒ¹é…æ¨¡å¼
    pattern = re.escape(sample)
    # å…è®¸æ•°å­—å˜åŒ–
    pattern = re.sub(r'\\d\+', r'\\d+', pattern)
    # å…è®¸å­—æ¯å˜åŒ–
    pattern = re.sub(r'[a-zA-Z]', r'[a-zA-Z]', pattern)
    
    return f"^{pattern}"

def smart_start_match(sample, text, regex):
    """æ™ºèƒ½å¼€å§‹åŒ¹é…"""
    if not sample or not text:
        return False
    
    # ç›´æ¥å­—ç¬¦ä¸²åŒ¹é…
    if sample in text:
        return True
    
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
    if regex and re.search(regex, text):
        return True
    
    # æ¨¡ç³ŠåŒ¹é…
    fuzzy_regex = get_fuzzy_regex_from_sample(sample)
    if fuzzy_regex and re.search(fuzzy_regex, text):
        return True
    
    return False

class PDFWordTableExtractor:
    def __init__(self):
        # é»˜è®¤çš„ç›®æ ‡åˆ—æ˜ å°„
        self.target_columns = {
            'åŠŸèƒ½æ¨¡å—': 'ä¸€çº§æ¨¡å—åç§°',
            'åŠŸèƒ½å­é¡¹': 'äºŒçº§æ¨¡å—åç§°', 
            'ä¸‰çº§æ¨¡å—': 'ä¸‰çº§æ¨¡å—åç§°',
            'åŠŸèƒ½æè¿°': 'åŠŸèƒ½æè¿°'
        }
        
        # è‡ªå®šä¹‰è¡¨å¤´æ˜ å°„
        self.custom_headers = {}
        
        # çŠ¶æ€å˜é‡
        self.current_lvl1 = ""
        self.current_lvl2 = ""
        self.current_lvl3 = ""
        self.current_description = []
        self.collected_data = []
        
    def extract_tables(self, file_path: str, file_type: str) -> List[Dict]:
        """æå–è¡¨æ ¼æ•°æ®çš„ä¸»å…¥å£"""
        if file_type == "pdf":
            return self.extract_tables_from_pdf_bid(file_path)
        elif file_type == "docx":
            if "åˆåŒ" in file_path:
                return self.extract_tables_from_word_contract(file_path)
            else:
                return self.extract_tables_from_word_bid(file_path)
        else:
            return []
    
    def setup_custom_headers(self):
        """è®¾ç½®è‡ªå®šä¹‰è¡¨å¤´æ˜ å°„"""
        self.custom_headers = {
            'åŠŸèƒ½æ¨¡å—': 'ä¸€çº§æ¨¡å—åç§°',
            'åŠŸèƒ½å­é¡¹': 'äºŒçº§æ¨¡å—åç§°',
            'ä¸‰çº§æ¨¡å—': 'ä¸‰çº§æ¨¡å—åç§°',
            'åŠŸèƒ½æè¿°': 'åˆåŒæè¿°'
        }
    
    def extract_tables_from_pdf_bid(self, pdf_path: str) -> List[Dict]:
        """æå–PDFæ ‡ä¹¦æ–‡ä»¶ä¸­çš„è¡¨æ ¼ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        # æ¸…ç©ºä¹‹å‰çš„çŠ¶æ€å˜é‡
        def clear_previous_state():
            self.current_lvl1 = ""
            self.current_lvl2 = ""
            self.current_lvl3 = ""
            self.current_description = []
            self.collected_data = []
        
        clear_previous_state()
        
        def reclassify_module(text, current_level):
            """é‡æ–°åˆ†ç±»æ¨¡å—"""
            if not text:
                return current_level
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸€çº§æ¨¡å—
            if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', text) or re.match(r'^\d+\.\d+\.\d+\.\d+', text):
                return 1
            # æ£€æŸ¥æ˜¯å¦æ˜¯äºŒçº§æ¨¡å—
            elif re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰)]', text) or re.match(r'^\d+\.\d+\.\d+\.\d+\.\d+', text):
                return 2
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‰çº§æ¨¡å—
            elif re.match(r'^\d+[ã€ï¼‰)]', text) or re.match(r'^\d+\.\d+\.\d+\.\d+\.\d+\.\d+', text):
                return 3
            else:
                return current_level
        
        def should_enable_verification():
            """æ˜¯å¦å¯ç”¨éªŒè¯"""
            return True
        
        def is_page_number(text):
            """æ£€æŸ¥æ˜¯å¦ä¸ºé¡µç """
            if not text:
                return False
            page_indicators = ['ç¬¬', 'é¡µ', 'Page', 'page']
            return any(indicator in str(text) for indicator in page_indicators)
        
        data = []
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text:
                        continue
                    
                    lines = text.split('\n')
                    current_level = 0
                    
                    for line in lines:
                        line = line.strip()
                        if not line or is_page_number(line):
                            continue
                        
                        # é‡æ–°åˆ†ç±»æ¨¡å—
                        new_level = reclassify_module(line, current_level)
                        if new_level != current_level:
                            # ä¿å­˜ä¹‹å‰çš„æ•°æ®
                            if self.current_description and (self.current_lvl1 or self.current_lvl2 or self.current_lvl3):
                                self.collected_data.append({
                                    'ä¸€çº§æ¨¡å—åç§°': self.current_lvl1,
                                    'äºŒçº§æ¨¡å—åç§°': self.current_lvl2,
                                    'ä¸‰çº§æ¨¡å—åç§°': self.current_lvl3,
                                    'æ ‡ä¹¦æè¿°': ' '.join(self.current_description),
                                    'åˆåŒæè¿°': '',
                                    'æ¥æºæ–‡ä»¶': os.path.basename(pdf_path)
                                })
                            
                            # æ›´æ–°å½“å‰çº§åˆ«
                            current_level = new_level
                            self.current_description = []
                            
                            if current_level == 1:
                                self.current_lvl1 = line
                                self.current_lvl2 = ""
                                self.current_lvl3 = ""
                            elif current_level == 2:
                                self.current_lvl2 = line
                                self.current_lvl3 = ""
                            elif current_level == 3:
                                self.current_lvl3 = line
                        else:
                            # æ”¶é›†æè¿°
                            if current_level > 0:
                                self.current_description.append(line)
                    
                    # å¤„ç†é¡µé¢æœ«å°¾çš„æ•°æ®
                    if self.current_description and (self.current_lvl1 or self.current_lvl2 or self.current_lvl3):
                        self.collected_data.append({
                            'ä¸€çº§æ¨¡å—åç§°': self.current_lvl1,
                            'äºŒçº§æ¨¡å—åç§°': self.current_lvl2,
                            'ä¸‰çº§æ¨¡å—åç§°': self.current_lvl3,
                            'æ ‡ä¹¦æè¿°': ' '.join(self.current_description),
                            'åˆåŒæè¿°': '',
                            'æ¥æºæ–‡ä»¶': os.path.basename(pdf_path)
                        })
                        self.current_description = []
        
        except Exception as e:
            logger.error(f"PDFå¤„ç†é”™è¯¯: {e}")
        
        return self.collected_data
    
    def extract_tables_from_pdf_bid_with_samples(self, pdf_path: str, lvl1_sample: str, 
                                               lvl2_sample: str = "", lvl3_sample: str = "", 
                                               end_sample: str = "") -> List[Dict]:
        """ä½¿ç”¨ç¼–å·æ ·ä¾‹æå–PDFæ ‡ä¹¦"""
        def clear_previous_state():
            self.current_lvl1 = ""
            self.current_lvl2 = ""
            self.current_lvl3 = ""
            self.current_description = []
            self.collected_data = []
        
        clear_previous_state()
        
        def reclassify_module(text, current_level):
            """é‡æ–°åˆ†ç±»æ¨¡å—"""
            if not text:
                return current_level
            
            # ä½¿ç”¨ç”¨æˆ·æä¾›çš„æ ·ä¾‹è¿›è¡ŒåŒ¹é…
            if lvl1_sample and smart_start_match(lvl1_sample, text, get_regex_from_sample(lvl1_sample)):
                return 1
            elif lvl2_sample and smart_start_match(lvl2_sample, text, get_regex_from_sample(lvl2_sample)):
                return 2
            elif lvl3_sample and smart_start_match(lvl3_sample, text, get_regex_from_sample(lvl3_sample)):
                return 3
            else:
                return current_level
        
        def should_enable_verification():
            """æ˜¯å¦å¯ç”¨éªŒè¯"""
            return True
        
        def is_page_number(text):
            """æ£€æŸ¥æ˜¯å¦ä¸ºé¡µç """
            if not text:
                return False
            page_indicators = ['ç¬¬', 'é¡µ', 'Page', 'page']
            return any(indicator in str(text) for indicator in page_indicators)
        
        def should_collect_description():
            """æ˜¯å¦åº”è¯¥æ”¶é›†æè¿°"""
            return self.current_lvl1 or self.current_lvl2 or self.current_lvl3
        
        data = []
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text:
                        continue
                    
                    lines = text.split('\n')
                    current_level = 0
                    
                    for line in lines:
                        line = line.strip()
                        if not line or is_page_number(line):
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦é‡åˆ°ç»“æŸæ ‡è®°
                        if end_sample and end_sample in line:
                            break
                        
                        # é‡æ–°åˆ†ç±»æ¨¡å—
                        new_level = reclassify_module(line, current_level)
                        if new_level != current_level:
                            # ä¿å­˜ä¹‹å‰çš„æ•°æ®
                            if self.current_description and should_collect_description():
                                self.collected_data.append({
                                    'ä¸€çº§æ¨¡å—åç§°': self.current_lvl1,
                                    'äºŒçº§æ¨¡å—åç§°': self.current_lvl2,
                                    'ä¸‰çº§æ¨¡å—åç§°': self.current_lvl3,
                                    'æ ‡ä¹¦æè¿°': ' '.join(self.current_description),
                                    'åˆåŒæè¿°': '',
                                    'æ¥æºæ–‡ä»¶': os.path.basename(pdf_path)
                                })
                            
                            # æ›´æ–°å½“å‰çº§åˆ«
                            current_level = new_level
                            self.current_description = []
                            
                            if current_level == 1:
                                self.current_lvl1 = line
                                self.current_lvl2 = ""
                                self.current_lvl3 = ""
                            elif current_level == 2:
                                self.current_lvl2 = line
                                self.current_lvl3 = ""
                            elif current_level == 3:
                                self.current_lvl3 = line
                        else:
                            # æ”¶é›†æè¿°
                            if should_collect_description():
                                self.current_description.append(line)
                    
                    # å¤„ç†é¡µé¢æœ«å°¾çš„æ•°æ®
                    if self.current_description and should_collect_description():
                        self.collected_data.append({
                            'ä¸€çº§æ¨¡å—åç§°': self.current_lvl1,
                            'äºŒçº§æ¨¡å—åç§°': self.current_lvl2,
                            'ä¸‰çº§æ¨¡å—åç§°': self.current_lvl3,
                            'æ ‡ä¹¦æè¿°': ' '.join(self.current_description),
                            'åˆåŒæè¿°': '',
                            'æ¥æºæ–‡ä»¶': os.path.basename(pdf_path)
                        })
                        self.current_description = []
        
        except Exception as e:
            logger.error(f"PDFå¤„ç†é”™è¯¯: {e}")
        
        return self.collected_data
    
    def extract_tables_from_pdf_contract_with_samples(self, pdf_path: str, lvl1_sample: str, 
                                                    lvl2_sample: str = "", lvl3_sample: str = "", 
                                                    end_sample: str = "") -> List[Dict]:
        """ä½¿ç”¨ç¼–å·æ ·ä¾‹æå–PDFåˆåŒ"""
        data = self.extract_tables_from_pdf_bid_with_samples(pdf_path, lvl1_sample, lvl2_sample, lvl3_sample, end_sample)
        
        # è°ƒæ•´æè¿°å­—æ®µæ˜ å°„
        for item in data:
            if item['æ ‡ä¹¦æè¿°']:
                item['åˆåŒæè¿°'] = item['æ ‡ä¹¦æè¿°']
                item['æ ‡ä¹¦æè¿°'] = ''
        
        return data
    
    def extract_tables_from_word_contract(self, docx_path: str, original_filename: str = None) -> List[Dict]:
        """æå–WordåˆåŒæ–‡ä»¶ä¸­çš„è¡¨æ ¼ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        data = []
        found_quotation_section = False
        current_headers = None
        doc = Document(docx_path)
        
        # å¦‚æœæ²¡æœ‰æä¾›åŸå§‹æ–‡ä»¶åï¼Œä½¿ç”¨docx_path
        if not original_filename:
            original_filename = os.path.basename(docx_path)
        
        # å…ˆæ£€æŸ¥æ•´ä¸ªæ–‡æ¡£æ˜¯å¦åŒ…å«åˆ†é¡¹æŠ¥ä»·è¡¨
        has_quotation_table = False
        for para in doc.paragraphs:
            if "åˆ†é¡¹æŠ¥ä»·è¡¨" in para.text:
                has_quotation_table = True
                break
        
        if not has_quotation_table:
            return data
        
        # å¤„ç†æ‰€æœ‰è¡¨æ ¼
        for table_idx, table in enumerate(doc.tables):
            rows = list(table.rows)
            if not rows:
                continue
            
            # æ£€æŸ¥è¡¨å¤´
            headers = [cell.text.strip().replace('\n', '') for cell in rows[0].cells]
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡è¡¨æ ¼
            if self._is_target_table_custom(headers):
                current_headers = headers
                found_quotation_section = True
                start_row = 1
                print(f"âœ… æ‰¾åˆ°åŒ¹é…çš„è¡¨æ ¼ï¼Œè¡¨å¤´ï¼š{headers}")
            elif current_headers and found_quotation_section:
                start_row = 0
            else:
                continue
                
            # å¤„ç†æ•°æ®è¡Œ
            for row_idx, row in enumerate(rows[start_row:], start=start_row):
                row_data = {}
                cells = row.cells
                
                # å¤„ç†åˆå¹¶å•å…ƒæ ¼çš„æƒ…å†µ
                for idx, header in enumerate(current_headers):
                    if idx < len(cells):
                        cell_text = cells[idx].text.strip()
                        row_data[header] = cell_text
                    else:
                        row_data[header] = ''
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                has_data = False
                
                # æ£€æŸ¥åºå·å­—æ®µ
                for header in current_headers:
                    if 'åºå·' in header and row_data.get(header, '').strip():
                        try:
                            int(row_data[header])
                            has_data = True
                            break
                        except ValueError:
                            pass
                
                # å¦‚æœåºå·ä¸æ˜¯æ•°å­—ï¼Œæ£€æŸ¥å…¶ä»–å…³é”®å­—æ®µ
                if not has_data:
                    key_fields = self._get_key_fields_for_check()
                    for field in key_fields:
                        for header in current_headers:
                            if field in header and row_data.get(header, '').strip():
                                has_data = True
                                break
                        if has_data:
                            break
                
                # å¦‚æœå…³é”®å­—æ®µéƒ½æ²¡æœ‰ï¼Œå†æ£€æŸ¥å…¶ä»–å­—æ®µ
                if not has_data:
                    has_data = any(row_data.get(header, '').strip() for header in current_headers)
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                if has_data:
                    mapped = self._map_word_row_custom(row_data, docx_path, original_filename)
                    
                    # æ£€æŸ¥é‡å¤å¹¶å¤„ç†
                    if len(data) > 0:
                        # æŸ¥æ‰¾ä¸Šä¸€ä¸ªéç©ºçš„ä¸€çº§æ¨¡å—åç§°
                        last_lvl1 = ""
                        last_lvl2 = ""
                        for i in range(len(data) - 1, -1, -1):
                            if data[i]['ä¸€çº§æ¨¡å—åç§°'].strip():
                                last_lvl1 = data[i]['ä¸€çº§æ¨¡å—åç§°']
                                break
                        for i in range(len(data) - 1, -1, -1):
                            if data[i]['äºŒçº§æ¨¡å—åç§°'].strip():
                                last_lvl2 = data[i]['äºŒçº§æ¨¡å—åç§°']
                                break
                        
                        # æ£€æŸ¥ä¸€çº§æ¨¡å—åç§°æ˜¯å¦é‡å¤
                        if mapped['ä¸€çº§æ¨¡å—åç§°'].strip() == last_lvl1.strip() and mapped['ä¸€çº§æ¨¡å—åç§°'].strip():
                            # å¦‚æœä¸€çº§æ¨¡å—åç§°é‡å¤ï¼Œæ¸…ç©ºå½“å‰çš„ä¸€çº§æ¨¡å—åç§°
                            mapped['ä¸€çº§æ¨¡å—åç§°'] = ""
                        
                        # æ£€æŸ¥äºŒçº§æ¨¡å—åç§°æ˜¯å¦é‡å¤
                        if mapped['äºŒçº§æ¨¡å—åç§°'].strip() == last_lvl2.strip() and mapped['äºŒçº§æ¨¡å—åç§°'].strip():
                            # å¦‚æœäºŒçº§æ¨¡å—åç§°é‡å¤ï¼Œæ¸…ç©ºå½“å‰çš„äºŒçº§æ¨¡å—åç§°
                            mapped['äºŒçº§æ¨¡å—åç§°'] = ""
                    
                    data.append(mapped)
        
        return data
    
    def extract_tables_from_word_bid(self, docx_path: str) -> List[Dict]:
        """æå–Wordæ ‡ä¹¦æ–‡ä»¶ä¸­çš„è¡¨æ ¼"""
        data = self.extract_tables_from_word_contract(docx_path)
        
        # è°ƒæ•´æè¿°å­—æ®µæ˜ å°„
        for item in data:
            if item['åˆåŒæè¿°']:
                item['æ ‡ä¹¦æè¿°'] = item['åˆåŒæè¿°']
                item['åˆåŒæè¿°'] = ''
        
        return data
    
    def _is_target_table_custom(self, headers: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡è¡¨æ ¼ï¼ˆè‡ªå®šä¹‰è¡¨å¤´ï¼‰"""
        if self.custom_headers:
            # ä½¿ç”¨è‡ªå®šä¹‰è¡¨å¤´
            found_headers = 0
            for custom_header in self.custom_headers.keys():
                for header in headers:
                    if custom_header in header:
                        found_headers += 1
                        break
            return found_headers >= 2
        else:
            # ä½¿ç”¨é»˜è®¤è¡¨å¤´
            return self._is_target_table(headers)
    
    def _get_key_fields_for_check(self) -> List[str]:
        """è·å–ç”¨äºæ£€æŸ¥çš„å…³é”®å­—æ®µåˆ—è¡¨"""
        if self.custom_headers:
            # ä½¿ç”¨è‡ªå®šä¹‰è¡¨å¤´
            return list(self.custom_headers.keys())
        else:
            # ä½¿ç”¨é»˜è®¤è¡¨å¤´
            return ['åŠŸèƒ½æè¿°', 'ä¸‰çº§æ¨¡å—', 'åŠŸèƒ½æ¨¡å—', 'åŠŸèƒ½å­é¡¹']
    
    def _map_word_row_custom(self, row_data: Dict, source_file: str, original_filename: str = None) -> Dict:
        """ä½¿ç”¨è‡ªå®šä¹‰è¡¨å¤´æ˜ å°„Wordè¡Œæ•°æ®"""
        if self.custom_headers:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” è‡ªå®šä¹‰è¡¨å¤´æ˜ å°„è°ƒè¯•:")
            print(f"   è‡ªå®šä¹‰è¡¨å¤´: {self.custom_headers}")
            print(f"   åŸå§‹è¡Œæ•°æ®: {row_data}")
            
            # ä½¿ç”¨è‡ªå®šä¹‰è¡¨å¤´æ˜ å°„
            mapped_data = {}
            for header, value in row_data.items():
                for custom_header, standard_field in self.custom_headers.items():
                    if custom_header in header:
                        mapped_data[standard_field] = value
                        print(f"   âœ… åŒ¹é…: {header} -> {standard_field} = {value}")
                        break
            
            print(f"   æ˜ å°„ç»“æœ: {mapped_data}")
            
            # ç¡®ä¿WordåˆåŒå†…å®¹æ˜ å°„åˆ°"åˆåŒæè¿°"
            desc_value = mapped_data.get('åˆåŒæè¿°', '')
            
            # å¦‚æœåˆåŒæè¿°ä¸ºç©ºï¼Œå°è¯•ä»åŸå§‹æ•°æ®ä¸­æŸ¥æ‰¾
            if not desc_value:
                print(f"   âš ï¸ åˆåŒæè¿°ä¸ºç©ºï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–æè¿°å­—æ®µ...")
                for header, value in row_data.items():
                    if value.strip() and ('æè¿°' in header or 'å¤‡æ³¨' in header or 'å†…å®¹' in header or 'åŠŸèƒ½' in header):
                        desc_value = value
                        print(f"   âœ… æ‰¾åˆ°æè¿°å­—æ®µ: {header} = {value}")
                        break
            
            mapped = {
                'ä¸€çº§æ¨¡å—åç§°': mapped_data.get('ä¸€çº§æ¨¡å—åç§°', ''),
                'äºŒçº§æ¨¡å—åç§°': mapped_data.get('äºŒçº§æ¨¡å—åç§°', ''),
                'ä¸‰çº§æ¨¡å—åç§°': mapped_data.get('ä¸‰çº§æ¨¡å—åç§°', ''),
                'æ ‡ä¹¦æè¿°': '',  # WordåˆåŒæ–‡ä»¶ï¼Œæ ‡ä¹¦æè¿°ä¸ºç©º
                'åˆåŒæè¿°': desc_value,  # WordåˆåŒå†…å®¹æ”¾è¿™é‡Œ
                'æ¥æºæ–‡ä»¶': original_filename if original_filename else (os.path.basename(source_file) if not source_file.endswith('tmp') else 'åˆåŒ.docx'),
            }
            
            print(f"   æœ€ç»ˆæ˜ å°„: {mapped}")
        else:
            # ä½¿ç”¨é»˜è®¤æ˜ å°„ï¼Œä½†å¢å¼ºåˆåŒæè¿°çš„å¤„ç†
            mapped = self._map_word_row(row_data, source_file)
            
            # å¦‚æœåˆåŒæè¿°ä¸ºç©ºï¼Œå°è¯•ä»åŸå§‹æ•°æ®ä¸­æŸ¥æ‰¾
            if not mapped['åˆåŒæè¿°']:
                for header, value in row_data.items():
                    if value.strip() and ('æè¿°' in header or 'å¤‡æ³¨' in header or 'å†…å®¹' in header or 'åŠŸèƒ½' in header):
                        mapped['åˆåŒæè¿°'] = value
                        break
        
        return mapped
    
    def _is_target_table(self, headers: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡è¡¨æ ¼ï¼ˆé»˜è®¤è¡¨å¤´ï¼‰"""
        target_headers = list(self.target_columns.keys())
        found_headers = []
        
        # é¢„å¤„ç†ç›®æ ‡å­—æ®µåï¼Œå»æ‰æ¢è¡Œç¬¦
        normalized_target_headers = {}
        for target in target_headers:
            normalized_target = target.replace('\n', '').replace('\\n', '')
            normalized_target_headers[normalized_target] = target
        
        for normalized_target, original_target in normalized_target_headers.items():
            for header_cell in headers:
                # å¯¹å®é™…å­—æ®µåä¹Ÿè¿›è¡Œé¢„å¤„ç†
                normalized_header = header_cell.replace('\n', '').replace('\\n', '')
                if normalized_target in normalized_header:
                    found_headers.append(original_target)
                    break
        
        return len(found_headers) >= 2
    
    def _map_word_row(self, row_data: Dict, source_file: str) -> Dict:
        """æ˜ å°„Wordè¡Œæ•°æ®åˆ°æ ‡å‡†æ ¼å¼"""
        # åˆ›å»ºæ ‡å‡†åŒ–å­—æ®µæ˜ å°„
        field_mapping = {}
        for original_field in ['åŠŸèƒ½æ¨¡å—', 'åŠŸèƒ½å­é¡¹', 'ä¸‰çº§æ¨¡å—', 'åŠŸèƒ½æè¿°']:
            normalized_field = original_field.replace('\n', '').replace('\\n', '')
            field_mapping[normalized_field] = original_field
        
        # æŸ¥æ‰¾åŒ¹é…çš„å­—æ®µ
        mapped_data = {}
        for header, value in row_data.items():
            normalized_header = header.replace('\n', '').replace('\\n', '')
            for normalized_field, original_field in field_mapping.items():
                if normalized_field in normalized_header:
                    mapped_data[original_field] = value
                    break
        
        mapped = {
            'ä¸€çº§æ¨¡å—åç§°': mapped_data.get('åŠŸèƒ½æ¨¡å—', ''),
            'äºŒçº§æ¨¡å—åç§°': mapped_data.get('åŠŸèƒ½å­é¡¹', ''),
            'ä¸‰çº§æ¨¡å—åç§°': mapped_data.get('ä¸‰çº§æ¨¡å—', ''),
            'æ ‡ä¹¦æè¿°': '',
            'åˆåŒæè¿°': '',
            'æ¥æºæ–‡ä»¶': os.path.basename(source_file) if not source_file.endswith('tmp') else 'åˆåŒ.docx',
        }
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å†³å®šå†…å®¹æ”¾å“ªä¸ªå­—æ®µ
        if "æ ‡ä¹¦" in source_file:
            mapped['æ ‡ä¹¦æè¿°'] = mapped_data.get('åŠŸèƒ½æè¿°', '')
        elif "åˆåŒ" in source_file:
            mapped['åˆåŒæè¿°'] = mapped_data.get('åŠŸèƒ½æè¿°', '')
        
        return mapped
    
    def _merge_paragraphs(self, desc_lines):
        """åˆå¹¶è‡ªç„¶æ®µ"""
        desc_paragraphs = []
        current_para = []
        for line in desc_lines:
            if not line.strip():
                if current_para:
                    desc_paragraphs.append(''.join(current_para))
                    current_para = []
            else:
                current_para.append(line.strip())
        if current_para:
            desc_paragraphs.append(''.join(current_para))
        return desc_paragraphs
    
    def _clean_extracted_data(self, data: List[Dict]) -> List[Dict]:
        """æ¸…ç†æå–çš„æ•°æ®ï¼Œè¿‡æ»¤æ— æ•ˆå†…å®¹"""
        cleaned_data = []
        
        for item in data:
            # æ£€æŸ¥æ˜¯å¦ä¸ºé¡µç ä¿¡æ¯
            def is_page_content(text):
                if not text:
                    return False
                page_indicators = ['ç¬¬', 'é¡µ', 'Page', 'page']
                return any(indicator in str(text) for indicator in page_indicators)
            
            # æ¸…ç†å„å­—æ®µ
            cleaned_item = {}
            for key, value in item.items():
                if is_page_content(value):
                    print(f"DEBUG: è¿‡æ»¤é¡µç å†…å®¹ '{value}' ä»å­—æ®µ '{key}'")
                    cleaned_item[key] = ""
                else:
                    cleaned_item[key] = value
            
            # åªä¿ç•™æœ‰æ•ˆæ•°æ®
            if any([cleaned_item.get("ä¸€çº§æ¨¡å—åç§°"), cleaned_item.get("äºŒçº§æ¨¡å—åç§°"), 
                   cleaned_item.get("ä¸‰çº§æ¨¡å—åç§°"), cleaned_item.get("æ ‡ä¹¦æè¿°"), cleaned_item.get("åˆåŒæè¿°")]):
                cleaned_data.append(cleaned_item)
        
        return cleaned_data
    
    def create_excel_output(self, data: List[Dict], output_path: str, append_mode=False):
        """åˆ›å»ºExcelè¾“å‡ºæ–‡ä»¶"""
        if not data:
            logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦è¾“å‡º")
            return None
        
        # æ–°å¢ï¼šæ™ºèƒ½åˆ†æ®µå¤„ç†å‡½æ•°
        def split_long_description(description, max_length=500):
            """æ™ºèƒ½åˆ†æ®µå¤„ç†é•¿æè¿°"""
            if not description or len(description) <= max_length:
                return [description]
            
            # é¢„å®šä¹‰ç¼–å·æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
            number_patterns = [
                # ä¸­æ–‡æ•°å­—æ ¼å¼
                r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€',  # ä¸€ã€äºŒã€ä¸‰ã€
                r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰)]',  # ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰
                r'[ï¼ˆ(]\d+[ï¼‰)]',  # ï¼ˆ1ï¼‰ï¼ˆ2ï¼‰
                r'\d+ã€',  # 1ã€2ã€
                r'\d+\.',  # 1.2.3.
                r'\d+ï¼‰',  # 1ï¼‰2ï¼‰
                r'\d+\)',  # 1)2)
                
                # è‹±æ–‡æ•°å­—æ ¼å¼
                r'\d+\.',  # 1.2.3.
                r'[a-z]+\)',  # a)b)c)
                r'[A-Z]+\.',  # A.B.C.
                r'[i]+\)',  # i)ii)iii)
            ]
            
            # å°è¯•æŒ‰ç¼–å·åˆ†å‰²
            for pattern in number_patterns:
                parts = re.split(f'({pattern})', description)
                if len(parts) > 1:
                    # é‡æ–°ç»„åˆåˆ†å‰²çš„éƒ¨åˆ†
                    result = []
                    current_part = ""
                    for i, part in enumerate(parts):
                        if re.match(pattern, part):
                            if current_part:
                                result.append(current_part.strip())
                            current_part = part
                        else:
                            current_part += part
                    
                    if current_part:
                        result.append(current_part.strip())
                    
                    # å¦‚æœåˆ†å‰²åçš„éƒ¨åˆ†ä»ç„¶å¤ªé•¿ï¼Œç»§ç»­åˆ†å‰²
                    final_result = []
                    for part in result:
                        if len(part) > max_length:
                            # æŒ‰å¥å·åˆ†å‰²
                            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', part)
                            current_sentence = ""
                            for sentence in sentences:
                                if len(current_sentence + sentence) <= max_length:
                                    current_sentence += sentence + "ã€‚"
                                else:
                                    if current_sentence:
                                        final_result.append(current_sentence.strip())
                                    current_sentence = sentence + "ã€‚"
                            if current_sentence:
                                final_result.append(current_sentence.strip())
                        else:
                            final_result.append(part)
                    
                    return final_result
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¼–å·æ ¼å¼ï¼ŒæŒ‰å¥å·åˆ†å‰²
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', description)
            result = []
            current_sentence = ""
            for sentence in sentences:
                if len(current_sentence + sentence) <= max_length:
                    current_sentence += sentence + "ã€‚"
                else:
                    if current_sentence:
                        result.append(current_sentence.strip())
                    current_sentence = sentence + "ã€‚"
            if current_sentence:
                result.append(current_sentence.strip())
            
            return result if result else [description]
        
        def get_unique_filename(filepath):
            """è·å–å”¯ä¸€æ–‡ä»¶å"""
            if not os.path.exists(filepath):
                return filepath
            
            base, ext = os.path.splitext(filepath)
            counter = 1
            while os.path.exists(f"{base}_{counter}{ext}"):
                counter += 1
            return f"{base}_{counter}{ext}"
        
        def clean_cell_value(value):
            """æ¸…ç†å•å…ƒæ ¼å€¼"""
            if not value:
                return ""
            
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            cleaned = re.sub(r'\s+', ' ', str(value).strip())
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            cleaned = re.sub(r'[^\w\s\u4e00-\u9fff.,ï¼Œã€‚ï¼ï¼Ÿï¼š:()ï¼ˆï¼‰\-]', '', cleaned)
            return cleaned
        
        # æ¸…ç†æ•°æ®
        cleaned_data = self._clean_extracted_data(data)
        
        if not cleaned_data:
            logger.warning("æ¸…ç†åæ²¡æœ‰æ•°æ®éœ€è¦è¾“å‡º")
            return None
        
        # å¤„ç†é•¿æè¿°
        processed_data = []
        for item in cleaned_data:
            processed_item = item.copy()
            
            # å¤„ç†æ ‡ä¹¦æè¿°
            if processed_item.get('æ ‡ä¹¦æè¿°'):
                desc_parts = split_long_description(processed_item['æ ‡ä¹¦æè¿°'])
                for i, part in enumerate(desc_parts):
                    if i == 0:
                        processed_item['æ ‡ä¹¦æè¿°'] = part
                    else:
                        # åˆ›å»ºæ–°è¡Œ
                        new_item = processed_item.copy()
                        new_item['æ ‡ä¹¦æè¿°'] = part
                        processed_data.append(new_item)
                        processed_item = None
                        break
                if processed_item:
                    processed_data.append(processed_item)
            else:
                processed_data.append(processed_item)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(processed_data)
        
        # ç¡®ä¿åˆ—é¡ºåºæ­£ç¡®
        column_order = ['ä¸€çº§æ¨¡å—åç§°', 'äºŒçº§æ¨¡å—åç§°', 'ä¸‰çº§æ¨¡å—åç§°', 'æ ‡ä¹¦æè¿°', 'åˆåŒæè¿°', 'æ¥æºæ–‡ä»¶']
        for col in column_order:
            if col not in df.columns:
                df[col] = ''
        df = df[column_order]
        
        # æ¸…ç†å•å…ƒæ ¼å€¼
        for col in df.columns:
            df[col] = df[col].apply(clean_cell_value)
        
        # è·å–å”¯ä¸€æ–‡ä»¶å
        output_path = get_unique_filename(output_path)
        
        # å†™å…¥Excel
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='æå–ç»“æœ', index=False)
            worksheet = writer.sheets['æå–ç»“æœ']
            
            # è®¾ç½®åˆ—å®½
            column_widths = {'A': 15, 'B': 15, 'C': 15, 'D': 45, 'E': 45, 'F': 10}
            for col, width in column_widths.items():
                worksheet.column_dimensions[col].width = width
            
            # è®¾ç½®è¡¨å¤´æ ·å¼
            header_font = Font(bold=True, size=12)
            header_alignment = Alignment(horizontal='center', vertical='center')
            for col in range(1, len(column_order) + 1):
                cell = worksheet.cell(row=1, column=col)
                cell.font = header_font
                cell.alignment = header_alignment
            
            # è®¾ç½®æ•°æ®è¡Œæ ·å¼å’Œè¡Œé«˜
            data_alignment = Alignment(horizontal='left', vertical='top', wrap_text=True)
            for row in range(2, len(df) + 2):
                # è®¡ç®—æ¯è¡Œæœ€å¤§å­—ç¬¦æ•°
                max_chars = 0
                for col in range(1, len(column_order) + 1):
                    cell_value = str(worksheet.cell(row=row, column=col).value or '')
                    lines = cell_value.split('\n')
                    for line in lines:
                        line_chars = len(line)
                        if line_chars > 30:
                            needed_lines = (line_chars // 30) + 1
                            max_chars = max(max_chars, needed_lines * 30)
                        else:
                            max_chars = max(max_chars, line_chars)
                
                # è®¡ç®—è¡Œé«˜
                estimated_lines = max(1, (max_chars // 30) + 1)
                row_height = max(20, estimated_lines * 18 + 10)
                worksheet.row_dimensions[row].height = row_height
                
                # è®¾ç½®å•å…ƒæ ¼å¯¹é½æ–¹å¼
                for col in range(1, len(column_order) + 1):
                    cell = worksheet.cell(row=row, column=col)
                    cell.alignment = data_alignment
            
            # è®¾ç½®è¾¹æ¡†
            thin_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            for row in worksheet.iter_rows(min_row=1, max_row=len(df) + 1, min_col=1, max_col=len(column_order)):
                for cell in row:
                    cell.border = thin_border
        
        return output_path

def main():
    """ä¸»å‡½æ•°ï¼ˆä¿ç•™ç”¨äºæµ‹è¯•ï¼‰"""
    extractor = PDFWordTableExtractor()
    print("PDFå’ŒWordè¡¨æ ¼æå–å·¥å…· - Webç‰ˆæœ¬")
    print("è¯·é€šè¿‡Streamlitåº”ç”¨ä½¿ç”¨æ­¤å·¥å…·")

if __name__ == "__main__":
    main()
    